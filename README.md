# Simple and Accurate Approach for Estimating Sample Complexity of Deep Neural Networks
The aim of the project is to estimate practical bounds for sample complexity for deep neural networks. The results presented in the paper are generated by following steps:

1) Fix train dataset, test dataset and model.
2) Train multiple copies of the model on different training data sizes.
3) Test each model on fixed test set. This will generate an empirical learning curve.
4) Save feature maps from the trained models.
5) Use feature maps to estimate the learning curve.

## Prerequisites
What things you need to install the software and how to install them
```
pytorch
torchvision
PIL
sklearn
scikit-learn
tensorboard (for logging metrics)
tqdm (for pretty progress bars)
```

## Training
```
--ms: manual seed (eg 123)

--img_per_class: images per class (used for imagenet) e.g 1000, 500 etc
--train_size: size of training data in % (used for all other data types) e.g 0.75, 0.25, 0.5 etc

--print_freq: frequency to print statistics
--mtype: model type
	choices are:
		"cifar_resnet18"
		"cifar_vgg16"
		"cifar_resnet50"
		"imagenet_resnet50"
		"lenet"
		"udacity_cnn"

--dtype: data type
	choices are:
		"cifar10"
		"cifar100"
		"mnist"
		"imagenet"
		"udacity"

--ep: total epochs 
--opt: optimizer
	"sgd"
	"adam"
--lr: learning rate
--wd: weight decay
--bs: batch size
--m: momentum
```
An example to train on cifar 10 data with 100% training data on resnet 18 model is as follows:
```
python train.py --dtype="cifar10" --train_size=1.0 \
 	--mtype="cifar_resnet18" --ep=500 --ms=123 --opt="sgd" --wd=1e-4 \
 	--lr=0.1 --bs=128 --print_freq=100 \
 	--n="cifar10_resnet18/cifar10_1.0/"
```
## Running the tests
An example to test on cifar 10 test data on resnet 18 model is as follows:
```
python test.py --dtype="cifar10" --modeltype="cifar_resnet18" \
	--bs=128 --cp_dir="checkpoints/cifar10_resnet18/"
```
The empirical learning curve data will saved in the folder "results/cifar10/"

## Estimating the learning curve


## Authors



## License


## Acknowledgments

